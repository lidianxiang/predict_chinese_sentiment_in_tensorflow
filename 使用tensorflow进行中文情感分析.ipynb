{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载必要的库\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba\n",
    "# gensim用来加载word vector\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings\n",
    "# bz2用来解压\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解压词向量\n",
    "with open('embeddings/sngs.zhihu.bigram', 'wb') as new_file, open('embeddings/sgns.zhihu.bigram.bz2','rb') as file:\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "        new_file.write(decompressor.decompress(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用gensim加载预训练中文分词embedding\n",
    "cn_model = KeyedVectors.load_word2vec_format('embeddings/sgns.zhihu.bigram', binary=False, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词向量的长度为:300\n"
     ]
    }
   ],
   "source": [
    "# 每个词的维度均为300维\n",
    "embedding_dim = cn_model['山东大学'].shape[0]\n",
    "print('词向量的长度为:{}'.format(embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.603470e-01,  3.677500e-01, -2.379650e-01,  5.301700e-02,\n",
       "       -3.628220e-01, -3.212010e-01, -1.903330e-01,  1.587220e-01,\n",
       "       -7.156200e-02, -4.625400e-02, -1.137860e-01,  3.515600e-01,\n",
       "       -6.408200e-02, -2.184840e-01,  3.286950e-01, -7.110330e-01,\n",
       "        1.620320e-01,  1.627490e-01,  5.528180e-01,  1.016860e-01,\n",
       "        1.060080e-01,  7.820700e-01, -7.537310e-01, -2.108400e-02,\n",
       "       -4.758250e-01, -1.130420e-01, -2.053000e-01,  6.624390e-01,\n",
       "        2.435850e-01,  9.171890e-01, -2.090610e-01, -5.290000e-02,\n",
       "       -7.969340e-01,  2.394940e-01, -9.028100e-02,  1.537360e-01,\n",
       "       -4.003980e-01, -2.456100e-02, -1.717860e-01,  2.037790e-01,\n",
       "       -4.344710e-01, -3.850430e-01, -9.366000e-02,  3.775310e-01,\n",
       "        2.659690e-01,  8.879800e-02,  2.493440e-01,  4.914900e-02,\n",
       "        5.996000e-03,  3.586430e-01, -1.044960e-01, -5.838460e-01,\n",
       "        3.093280e-01, -2.828090e-01, -8.563400e-02, -5.745400e-02,\n",
       "       -2.075230e-01,  2.845980e-01,  1.414760e-01,  1.678570e-01,\n",
       "        1.957560e-01,  7.782140e-01, -2.359000e-01, -6.833100e-02,\n",
       "        2.560170e-01, -6.906900e-02, -1.219620e-01,  2.683020e-01,\n",
       "        1.678810e-01,  2.068910e-01,  1.987520e-01,  6.720900e-02,\n",
       "       -3.975290e-01, -7.123140e-01,  5.613200e-02,  2.586000e-03,\n",
       "        5.616910e-01,  1.157000e-03, -4.341190e-01,  1.977480e-01,\n",
       "        2.519540e-01,  8.835000e-03, -3.554600e-01, -1.573500e-02,\n",
       "       -2.526010e-01,  9.355900e-02, -3.962500e-02, -1.628350e-01,\n",
       "        2.980950e-01,  1.647900e-01, -5.454270e-01,  3.888790e-01,\n",
       "        1.446840e-01, -7.239600e-02, -7.597800e-02, -7.803000e-03,\n",
       "        2.020520e-01, -4.424750e-01,  3.911580e-01,  2.115100e-01,\n",
       "        6.516760e-01,  5.668030e-01,  5.065500e-02, -1.259650e-01,\n",
       "       -3.720640e-01,  2.330470e-01,  6.659900e-02,  8.300600e-02,\n",
       "        2.540460e-01, -5.279760e-01, -3.843280e-01,  3.366460e-01,\n",
       "        2.336500e-01,  3.564750e-01, -4.884160e-01, -1.183910e-01,\n",
       "        1.365910e-01,  2.293420e-01, -6.151930e-01,  5.212050e-01,\n",
       "        3.412000e-01,  5.757940e-01,  2.354480e-01, -3.641530e-01,\n",
       "        7.373400e-02,  1.007380e-01, -3.211410e-01, -3.040480e-01,\n",
       "       -3.738440e-01, -2.515150e-01,  2.633890e-01,  3.995490e-01,\n",
       "        4.461880e-01,  1.641110e-01,  1.449590e-01, -4.191540e-01,\n",
       "        2.297840e-01,  6.710600e-02,  3.316430e-01, -6.026500e-02,\n",
       "       -5.130610e-01,  1.472570e-01,  2.414060e-01,  2.011000e-03,\n",
       "       -3.823410e-01, -1.356010e-01,  3.112300e-01,  9.177830e-01,\n",
       "       -4.511630e-01,  1.272190e-01, -9.431600e-02, -8.216000e-03,\n",
       "       -3.835440e-01,  2.589400e-02,  6.374980e-01,  4.931630e-01,\n",
       "       -1.865070e-01,  4.076900e-01, -1.841000e-03,  2.213160e-01,\n",
       "        2.253950e-01, -2.159220e-01, -7.611480e-01, -2.305920e-01,\n",
       "        1.296890e-01, -1.304100e-01, -4.742270e-01,  2.275500e-02,\n",
       "        4.255050e-01,  1.570280e-01,  2.975300e-02,  1.931830e-01,\n",
       "        1.304340e-01, -3.179800e-02,  1.516650e-01, -2.154310e-01,\n",
       "       -4.681410e-01,  1.007326e+00, -6.698940e-01, -1.555240e-01,\n",
       "        1.797170e-01,  2.848660e-01,  6.216130e-01,  1.549510e-01,\n",
       "        6.225000e-02, -2.227800e-02,  2.561270e-01, -1.006380e-01,\n",
       "        2.807900e-02,  4.597710e-01, -4.077750e-01, -1.777390e-01,\n",
       "        1.920500e-02, -4.829300e-02,  4.714700e-02, -3.715200e-01,\n",
       "       -2.995930e-01, -3.719710e-01,  4.622800e-02, -1.436460e-01,\n",
       "        2.532540e-01, -9.334000e-02, -4.957400e-02, -3.803850e-01,\n",
       "        5.970110e-01,  3.578450e-01, -6.826000e-02,  4.735200e-02,\n",
       "       -3.707590e-01, -8.621300e-02, -2.556480e-01, -5.950440e-01,\n",
       "       -4.757790e-01,  1.079320e-01,  9.858300e-02,  8.540300e-01,\n",
       "        3.518370e-01, -1.306360e-01, -1.541590e-01,  1.166775e+00,\n",
       "        2.048860e-01,  5.952340e-01,  1.158830e-01,  6.774400e-02,\n",
       "        6.793920e-01, -3.610700e-01,  1.697870e-01,  4.118530e-01,\n",
       "        4.731000e-03, -7.516530e-01, -9.833700e-02, -2.312220e-01,\n",
       "       -7.043300e-02,  1.576110e-01, -4.780500e-02, -7.344390e-01,\n",
       "       -2.834330e-01,  4.582690e-01,  3.957010e-01, -8.484300e-02,\n",
       "       -3.472550e-01,  1.291660e-01,  3.838960e-01, -3.287600e-02,\n",
       "       -2.802220e-01,  5.257030e-01, -3.609300e-02, -4.842220e-01,\n",
       "        3.690700e-02,  3.429560e-01,  2.902490e-01, -1.624650e-01,\n",
       "       -7.513700e-02,  2.669300e-01,  5.778230e-01, -3.074020e-01,\n",
       "       -2.183790e-01, -2.834050e-01,  1.350870e-01,  1.490070e-01,\n",
       "        1.438400e-02, -2.509040e-01, -3.376100e-01,  1.291880e-01,\n",
       "       -3.808700e-01, -4.420520e-01, -2.512300e-01, -1.328990e-01,\n",
       "       -1.211970e-01,  2.532660e-01,  2.757050e-01, -3.382040e-01,\n",
       "        1.178070e-01,  3.860190e-01,  5.277960e-01,  4.581920e-01,\n",
       "        1.502310e-01,  1.226320e-01,  2.768540e-01, -4.502080e-01,\n",
       "       -1.992670e-01,  1.689100e-02,  1.188860e-01,  3.502440e-01,\n",
       "       -4.064770e-01,  2.610280e-01, -1.934990e-01, -1.625660e-01,\n",
       "        2.498400e-02, -1.867150e-01, -1.954400e-02, -2.281900e-01,\n",
       "       -3.417670e-01, -5.222770e-01, -9.543200e-02, -3.500350e-01,\n",
       "        2.154600e-02,  2.318040e-01,  5.395310e-01, -4.223720e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 300维的向量\n",
    "cn_model['山东大学']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66128117"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算相似度\n",
    "cn_model.similarity('橘子', '橙子')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6612812"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 上述的相似度为余弦相似度，实质上dot（'橘子'/|'橘子'|， '橙子'/|'橙子'| ）\n",
    "np.dot(cn_model['橘子']/np.linalg.norm(cn_model['橘子']),\n",
    "       cn_model['橙子']/np.linalg.norm(cn_model['橙子']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('高中', 0.7247823476791382),\n",
       " ('本科', 0.676853597164154),\n",
       " ('研究生', 0.6244412660598755),\n",
       " ('中学', 0.6088205575942993),\n",
       " ('大学本科', 0.5959085822105408),\n",
       " ('初中', 0.5883588790893555),\n",
       " ('读研', 0.5778335332870483),\n",
       " ('职高', 0.5767995715141296),\n",
       " ('大学毕业', 0.5767451524734497),\n",
       " ('师范大学', 0.5708829164505005)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找出最相近的词\n",
    "cn_model.most_similar(positive=['大学'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在老师 会计师 程序员 律师 医生 老人中:\n",
      "不是一类的词是:老人\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lidianxiang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "# 找出不同的词\n",
    "test_words = '老师 会计师 程序员 律师 医生 老人'\n",
    "test_words_result = cn_model.doesnt_match(test_words.split())\n",
    "print('在{}中:\\n不是一类的词是:{}'.format(test_words, test_words_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('出轨', 0.6100173592567444),\n",
       " ('脚踏两只船', 0.5825897455215454),\n",
       " ('打胎', 0.5778396129608154),\n",
       " ('移情别恋', 0.5618439316749573),\n",
       " ('未婚先孕', 0.5577656626701355)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.most_similar(positive=['女人','劈腿'],negative=['男人'],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总数为：4000\n"
     ]
    }
   ],
   "source": [
    "# 加载训练语料\n",
    "# 将语料.zip解压后得到两个文件夹：pos,neg\n",
    "import os\n",
    "pos_txts = os.listdir('./语料/pos/')\n",
    "neg_txts = os.listdir('./语料/neg/')\n",
    "print('样本总数为：{}'.format(len(pos_txts) + len(neg_txts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_orig = []\n",
    "train_target = []\n",
    "with open('positive_samples.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        dic = eval(line)\n",
    "        train_texts_orig.append(dic['text'])\n",
    "        train_target.append(dic['label'])\n",
    "        \n",
    "with open('negative_samples.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        dic = eval(line)\n",
    "        train_texts_orig.append(dic['text'])\n",
    "        train_target.append(dic['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用tensorflow的keras接口来建模\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/pw/f22wybpj5zd97xm6dpkl7_fh0000gn/T/jieba.cache\n",
      "Loading model cost 0.689 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 分词和tokenize\n",
    "train_tokens = []\n",
    "for text in train_texts_orig:\n",
    "    #text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、～@#])]\")\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # jieba 分词\n",
    "    cut = jieba.cut(text)\n",
    "    # 将生成器转为list列表\n",
    "    cut_list = [i for i in cut]\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            # 将词转换为suoyin\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # 若词不在字典中，则输出零\n",
    "            cut_list[i] = 0\n",
    "    train_tokens.append(cut_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.4495"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获得所有tokens的长度\n",
    "num_tokens = [len(token) for token in train_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 平均token的长度\n",
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1540"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最长的tokens的长度\n",
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log(num_tokens), bins=100)\n",
    "plt.xlim((0, 10))\n",
    "plt.xlabel('length of tokens')\n",
    "plt.ylabel('number of tokens')\n",
    "plt.title('Distribution of tokens length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens平均值加上两个tokens的标准差\n",
    "# 假设tokens的长度的分布为正态分布，则max_tokens这个值可以覆盖到95%左右的样本\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9565"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens的长度为236时，大约可以覆盖到95%\n",
    "# 对于长度不足的样本进行padding，超长的进行修剪\n",
    "np.sum(num_tokens < max_tokens) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向tokenize\n",
    "# 定义一个函数，用来将索引转换成可阅读的文本，这个对debug很重要\n",
    "def reverse_tokens(tokens):\n",
    "    text = ''\n",
    "    for i in tokens:\n",
    "        if i != 0:\n",
    "            text = text + cn_model.index2word[i]\n",
    "        else:\n",
    "            text = text + ' '\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'早餐太差无论去多少人那边也不加食品的酒店应该重视一下这个问题了房间本身很好'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse = reverse_tokens(train_tokens[0])\n",
    "reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。\\n\\n房间本身很好。'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始文本\n",
    "train_texts_orig[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备embedding_matrix\n",
    "# 这里因为样本只有4000，所以值考虑300维\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只使用前50000个词\n",
    "num_words = 50000\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrix为一个[num_words, embedding_dim]的矩阵\n",
    "# 维度为50000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查index是否对应\n",
    "# 输出300的意义为长度为300的embedding向量一一对应\n",
    "np.sum(cn_model[cn_model.index2word[333]] == embedding_matrix[333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_matrix的维度\n",
    "# 这个维度为keras的要求，后续会在模型中用到\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行padding和truncating\n",
    "# 输入的train_tokens是一个list\n",
    "# 返回的train_pad是一个numpy_array\n",
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                         padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超过50000词向量的词用0代替\n",
    "train_pad[train_pad >= num_words] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         290,  3053,    57,   169,    73,     1,    25, 11216,    49,\n",
       "         163, 15985,     0,     0,    30,     8,     0,     1,   228,\n",
       "         223,    40,    35,   653,     0,     5,  1642,    29, 11216,\n",
       "        2751,   500,    98,    30,  3159,  2225,  2146,   371,  6285,\n",
       "         169, 27396,     1,  1191,  5432,  1080, 20055,    57,   562,\n",
       "           1, 22671,    40,    35,   169,  2567,     0, 42665,  7761,\n",
       "         110,     0,     0, 41281,     0,   110,     0, 35891,   110,\n",
       "           0, 28781,    57,   169,  1419,     1, 11670,     0, 19470,\n",
       "           1,     0,     0,   169, 35071,    40,   562,    35, 12398,\n",
       "         657,  4857], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可见padding之后前面的tokens全变成0，文本在最后面\n",
    "train_pad[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备target向量，前2000样本为1，后2000样本为0\n",
    "train_target = np.array(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行训练和测试的样本的分割\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90%的样本用来进行训练，剩余10%用来进行测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
    "                                                   train_target,\n",
    "                                                   test_size=0.1,\n",
    "                                                   random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                        房间很大还有海景阳台走出酒店就是沙滩非常不错唯一遗憾的就是不能刷 不方便\n",
      "class: 1\n"
     ]
    }
   ],
   "source": [
    "# 查看训练样本\n",
    "print(reverse_tokens(X_train[35]))\n",
    "print('class:', y_train[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用keras搭建LSTM模型\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                   embedding_dim,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=max_tokens,\n",
    "                   trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(LSTM(units=16, return_sequences=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))\n",
    "optimizer = Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=optimizer,\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 236, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 236, 128)          186880    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 16)                9280      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 15,196,177\n",
      "Trainable params: 196,177\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "path_checkpoint = 'sentiment_checkpoint.keras'\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
    "                            verbose=1, save_weights_only=True,\n",
    "                            save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试加载已训练的模型\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义early_stopping\n",
    "# 如果3个epoch内validation_loss没有改善则停止训练\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动降低learing_rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                factor=0.1, min_lr=1e-8, patience=0,\n",
    "                                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义callback函数\n",
    "callbacks = [\n",
    "    earlystopping,\n",
    "    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3240 samples, validate on 360 samples\n",
      "Epoch 1/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.2988 - acc: 0.8750\n",
      "Epoch 00001: val_loss improved from inf to 0.30273, saving model to sentiment_checkpoint.keras\n",
      "3240/3240 [==============================] - 19s 6ms/step - loss: 0.2992 - acc: 0.8744 - val_loss: 0.3027 - val_acc: 0.8861\n",
      "Epoch 2/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.2538 - acc: 0.9016\n",
      "Epoch 00002: val_loss did not improve from 0.30273\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "3240/3240 [==============================] - 17s 5ms/step - loss: 0.2545 - acc: 0.9012 - val_loss: 0.3451 - val_acc: 0.8667\n",
      "Epoch 3/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9169\n",
      "Epoch 00003: val_loss did not improve from 0.30273\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "3240/3240 [==============================] - 18s 5ms/step - loss: 0.2271 - acc: 0.9170 - val_loss: 0.3375 - val_acc: 0.8667\n",
      "Epoch 4/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9222\n",
      "Epoch 00004: val_loss did not improve from 0.30273\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "3240/3240 [==============================] - 18s 6ms/step - loss: 0.2150 - acc: 0.9228 - val_loss: 0.3308 - val_acc: 0.8722\n",
      "Epoch 5/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.2125 - acc: 0.9250\n",
      "Epoch 00005: val_loss did not improve from 0.30273\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "3240/3240 [==============================] - 18s 6ms/step - loss: 0.2135 - acc: 0.9247 - val_loss: 0.3308 - val_acc: 0.8750\n",
      "Epoch 6/20\n",
      "3200/3240 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9250\n",
      "Epoch 00006: val_loss did not improve from 0.30273\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "3240/3240 [==============================] - 18s 5ms/step - loss: 0.2132 - acc: 0.9247 - val_loss: 0.3308 - val_acc: 0.8750\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f99e1d8d588>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练\n",
    "model.fit(X_train, y_train,\n",
    "         validation_split=0.1,\n",
    "         epochs=20,\n",
    "         batch_size=128,\n",
    "         callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 4ms/step\n",
      "Accuracy:87.00%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    print(text)\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 分词\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [ i for i in cut ]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "            if cut_list[i] >= 50000:\n",
    "                cut_list[i] = 0\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                              padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('是一例正面评价', 'output=%.2f' % coef)\n",
    "    else:\n",
    "        print('是一例负面评价', 'output=%.2f' % coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "酒店设施不是新的，服务态度很不好\n",
      "是一例负面评价 output=0.08\n",
      "酒店卫生条件非常不好\n",
      "是一例负面评价 output=0.07\n",
      "床铺非常舒适\n",
      "是一例正面评价 output=0.87\n",
      "房间很凉，不给开暖气\n",
      "是一例负面评价 output=0.13\n",
      "房间很凉爽，空调冷气很足\n",
      "是一例正面评价 output=0.76\n",
      "酒店环境不好，住宿体验很不好\n",
      "是一例负面评价 output=0.05\n",
      "房间隔音不到位\n",
      "是一例负面评价 output=0.13\n",
      "晚上回来发现没有打扫卫生\n",
      "是一例负面评价 output=0.26\n",
      "因为过节所以要我临时加钱，比团购的价格贵\n",
      "是一例负面评价 output=0.04\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    '酒店设施不是新的，服务态度很不好',\n",
    "    '酒店卫生条件非常不好',\n",
    "    '床铺非常舒适',\n",
    "    '房间很凉，不给开暖气',\n",
    "    '房间很凉爽，空调冷气很足',\n",
    "    '酒店环境不好，住宿体验很不好',\n",
    "    '房间隔音不到位' ,\n",
    "    '晚上回来发现没有打扫卫生',\n",
    "    '因为过节所以要我临时加钱，比团购的价格贵'\n",
    "]\n",
    "for text in test_list:\n",
    "    predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.T[0]\n",
    "y_pred = [1 if p >=0.5 else 0 for p in y_pred]\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出错误分类的索引\n",
    "misclassified = np.where(y_pred != y_actual)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "# 输出所有错误分类的索引\n",
    "len(misclassified)\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                        由于2007年 有一些新问题可能还没来得及解决我因为工作需要经常要住那里所以慎重的提出以下 ：1 后 的 淋浴喷头的位置都太高我换了房间还是一样很不好用2 后的一些管理和服务还很不到位尤其是前台入住和 时代效率太低每次 都超过10分钟好像不符合 宾馆的要求\n",
      "预测的分类 0\n",
      "实际的分类 1\n"
     ]
    }
   ],
   "source": [
    "# 找出错误分类的样本\n",
    "idx = 101\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
